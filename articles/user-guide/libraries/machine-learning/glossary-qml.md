---
title: Słownik biblioteki Quantum Machine Learning
author: alexeib2
ms.author: alexei.bocharov@microsoft.com
ms.date: 2/27/2020
ms.topic: article
uid: microsoft.quantum.libraries.machine-learning.training
no-loc:
- Q#
- $$v
ms.openlocfilehash: 068fc61d0d7c066df1270384679e13a3b3a8c878
ms.sourcegitcommit: 75c4edc7c410cc63dc8352e2a5bef44b433ed188
ms.translationtype: MT
ms.contentlocale: pl-PL
ms.lasthandoff: 08/25/2020
ms.locfileid: "88863025"
---
# <a name="quantum-machine-learning-glossary"></a><span data-ttu-id="ff5d9-102">Słownik Quantum Machine Learning</span><span class="sxs-lookup"><span data-stu-id="ff5d9-102">Quantum Machine Learning glossary</span></span>

<span data-ttu-id="ff5d9-103">Uczenie klasyfikatora opartego na obwodach jest procesem z wieloma ruchomymi częściami, które wymagają tego samego (lub nieco większego) stopnia kalibracji w okresie próbnym i błędów jako szkolenia tradycyjnych klasyfikatorów.</span><span class="sxs-lookup"><span data-stu-id="ff5d9-103">Training of a circuit-centric quantum classifier is a process with many moving parts that require the same (or slightly larger) amount of calibration by trial and error as training of traditional classifiers.</span></span> <span data-ttu-id="ff5d9-104">Tutaj definiujemy główne koncepcje i składniki tego procesu szkoleniowego.</span><span class="sxs-lookup"><span data-stu-id="ff5d9-104">Here we define the main concepts and ingredients of this training process.</span></span>

## <a name="trainingtesting-schedules"></a><span data-ttu-id="ff5d9-105">Harmonogramy szkoleń/testów</span><span class="sxs-lookup"><span data-stu-id="ff5d9-105">Training/testing schedules</span></span>

<span data-ttu-id="ff5d9-106">W kontekście szkolenia klasyfikatora *harmonogram* zawiera opis podzestawu próbek danych w ramach ogólnego szkolenia lub zestawu testów.</span><span class="sxs-lookup"><span data-stu-id="ff5d9-106">In the context of classifier training a *schedule* describes a subset of data samples in an overall training or testing set.</span></span> <span data-ttu-id="ff5d9-107">Harmonogram jest zwykle definiowany jako kolekcja przykładowych indeksów.</span><span class="sxs-lookup"><span data-stu-id="ff5d9-107">A schedule is usually defined as a collection of sample indices.</span></span>

## <a name="parameterbias-scores"></a><span data-ttu-id="ff5d9-108">Wyniki parametru/Bias</span><span class="sxs-lookup"><span data-stu-id="ff5d9-108">Parameter/bias scores</span></span>

<span data-ttu-id="ff5d9-109">W przypadku wektora parametrów kandydujących i odchylenia klasyfikatora ich *wyniki sprawdzania poprawności* są mierzone względem wybranego harmonogramu walidacji i są wyrażane przez wiele nieprawidłowych klasyfikacji na wszystkich przykładach w harmonogramie s.</span><span class="sxs-lookup"><span data-stu-id="ff5d9-109">Given a candidate parameter vector and a classifier bias, their *validation score* is measured relative to a chosen validation schedule S and is expressed by a number of misclassifications counted over all the samples in the schedule S.</span></span>

## <a name="hyperparameters"></a><span data-ttu-id="ff5d9-110">Hiperparametry</span><span class="sxs-lookup"><span data-stu-id="ff5d9-110">Hyperparameters</span></span>

<span data-ttu-id="ff5d9-111">Proces szkolenia modelu podlega określonym wstępnie określonym wartościom, które są nazywane *parametrami*:</span><span class="sxs-lookup"><span data-stu-id="ff5d9-111">The model training process is governed by certain pre-set values called *hyperparameters*:</span></span>

### <a name="learning-rate"></a><span data-ttu-id="ff5d9-112">Tempo nauki</span><span class="sxs-lookup"><span data-stu-id="ff5d9-112">Learning rate</span></span>

<span data-ttu-id="ff5d9-113">Jest to jeden z najważniejszych parametrów.</span><span class="sxs-lookup"><span data-stu-id="ff5d9-113">It is one of the key hyperparameters.</span></span> <span data-ttu-id="ff5d9-114">Definiuje, ile bieżącego oszacowania gradientu stochastycznego ma wpływ na aktualizację parametru.</span><span class="sxs-lookup"><span data-stu-id="ff5d9-114">It defines how much current stochastic gradient estimate impacts the parameter update.</span></span> <span data-ttu-id="ff5d9-115">Rozmiar różnic między aktualizacjami parametrów jest proporcjonalny do stawki szkoleniowej.</span><span class="sxs-lookup"><span data-stu-id="ff5d9-115">The size of parameter update delta is proportional to the learning rate.</span></span> <span data-ttu-id="ff5d9-116">Mniejsze wartości szybkości uczenia prowadzą do wolniejszego ewolucji parametrów i wolniejszych zbieżności, ale nadmiernie duże wartości LR mogą spowodować przerwanie zbieżności, gdy gradient nie jest nigdy zatwierdzany do określonego minimum lokalnego.</span><span class="sxs-lookup"><span data-stu-id="ff5d9-116">Smaller learning rate values lead to slower parameter evolution and slower convergence, but excessively large values of LR may break the convergence altogether as the gradient descent never commits to a particular local minimum.</span></span> <span data-ttu-id="ff5d9-117">Częstotliwość uczenia jest dostosowywana do pewnego stopnia przez algorytm szkoleniowy, dlatego należy wybrać dobrą wartość początkową.</span><span class="sxs-lookup"><span data-stu-id="ff5d9-117">While learning rate is adaptively adjusted by the training algorithm to some extent, selecting a good initial value for it is important.</span></span> <span data-ttu-id="ff5d9-118">Normalna wartość domyślna dla stawki szkoleniowej to 0,1.</span><span class="sxs-lookup"><span data-stu-id="ff5d9-118">A usual default initial value for learning rate is 0.1.</span></span> <span data-ttu-id="ff5d9-119">Wybór najlepszej wartości stawki szkoleniowej to świetna część (patrz na przykład sekcja 4,3 of Goodfellow et al., "Uczenie głębokie", MIT, 2017).</span><span class="sxs-lookup"><span data-stu-id="ff5d9-119">Selecting the best value of learning rate is a fine art (see, for example, section 4.3 of Goodfellow et al.,"Deep learning", MIT Press, 2017).</span></span>

### <a name="minibatch-size"></a><span data-ttu-id="ff5d9-120">Rozmiar Minibatch</span><span class="sxs-lookup"><span data-stu-id="ff5d9-120">Minibatch size</span></span>

<span data-ttu-id="ff5d9-121">Definiuje liczbę próbek danych używanych do pojedynczego oszacowania gradientu stochastycznego.</span><span class="sxs-lookup"><span data-stu-id="ff5d9-121">Defines how many data samples is used for a single estimation of stochastic gradient.</span></span> <span data-ttu-id="ff5d9-122">Większe wartości rozmiaru minibatch zazwyczaj prowadzą do bardziej niezawodnej i większej spójności monotoniczny, ale mogą spowodować spowolnienie procesu szkolenia, ponieważ koszt dowolnego oszacowania gradientu jest proporcjonalny do rozmiaru minimatch.</span><span class="sxs-lookup"><span data-stu-id="ff5d9-122">Larger values of minibatch size generally lead to more robust and more monotonic convergence but can potentially slow down the training process, as the cost of any one gradient estimation is proportional to the minimatch size.</span></span> <span data-ttu-id="ff5d9-123">Normalna wartość domyślna dla rozmiaru minibatch wynosi 10.</span><span class="sxs-lookup"><span data-stu-id="ff5d9-123">A usual default value for the minibatch size is 10.</span></span>

### <a name="training-epochs-tolerance-gridlocks"></a><span data-ttu-id="ff5d9-124">Treningi, tolerancja, gridlocks</span><span class="sxs-lookup"><span data-stu-id="ff5d9-124">Training epochs, tolerance, gridlocks</span></span>

<span data-ttu-id="ff5d9-125">"Epoka" oznacza jedno kompletne przejście przez zaplanowane dane szkoleniowe.</span><span class="sxs-lookup"><span data-stu-id="ff5d9-125">"Epoch" means one complete pass through the scheduled training data.</span></span>
<span data-ttu-id="ff5d9-126">Maksymalna liczba epok na wątek szkoleniowy (patrz poniżej) powinna być ograniczona.</span><span class="sxs-lookup"><span data-stu-id="ff5d9-126">The maximum number of epochs per a training thread (see below) should be capped.</span></span> <span data-ttu-id="ff5d9-127">Wątek szkoleniowy jest zdefiniowany do zakończenia (z najlepszymi znanymi parametrami kandydatów), gdy została wykonana Maksymalna liczba epok.</span><span class="sxs-lookup"><span data-stu-id="ff5d9-127">The training thread is defined to terminate (with the best known candidate parameters) when the maximum number of epochs has been executed.</span></span> <span data-ttu-id="ff5d9-128">Jednak takie szkolenie zostanie zakończone wcześniej, gdy wartość w harmonogramie sprawdzania poprawności nie będzie mniejsza niż wybrana tolerancja.</span><span class="sxs-lookup"><span data-stu-id="ff5d9-128">However such training would terminate earlier when misclassification rate on validation schedule falls below a chosen tolerance.</span></span> <span data-ttu-id="ff5d9-129">Załóżmy na przykład, że niedozwolona tolerancja klasyfikacji wynosi 0,01 (1%); Jeśli na 2000 zestawie walidacji próbek jest wyświetlana mniej niż 20 błędów klasyfikacji, osiągnięto poziom tolerancji.</span><span class="sxs-lookup"><span data-stu-id="ff5d9-129">Suppose, for example, that misclassification tolerance is 0.01 (1%); if on validation set of 2000 samples we are seeing fewer than 20 misclassifications, then the tolerance level has been achieved.</span></span> <span data-ttu-id="ff5d9-130">Wątek szkoleniowy kończy się również przedwcześnie, jeśli wynik sprawdzania poprawności modelu kandydata nie pokazał żadnych ulepszeń w przypadku kilku kolejnych epok (korków).</span><span class="sxs-lookup"><span data-stu-id="ff5d9-130">A training thread also terminates prematurely if the validation score of the candidate model has not shown any improvement over several consecutive epochs (a gridlock).</span></span> <span data-ttu-id="ff5d9-131">Logika dla zakończenia korków jest obecnie stałe.</span><span class="sxs-lookup"><span data-stu-id="ff5d9-131">The logic for the gridlock termination is currently hardcoded.</span></span>

### <a name="measurements-count"></a><span data-ttu-id="ff5d9-132">Liczba pomiarów</span><span class="sxs-lookup"><span data-stu-id="ff5d9-132">Measurements count</span></span>

<span data-ttu-id="ff5d9-133">Oszacowanie wyników szkolenia/sprawdzania poprawności oraz składników gradientu stochastycznego na urządzeniu Quantum powoduje oszacowanie nakładających się Stanów Quantum, które wymagają wielu pomiarów odpowiednich observables.</span><span class="sxs-lookup"><span data-stu-id="ff5d9-133">Estimating the training/validation scores and the components of the stochastic gradient on a quantum device amounts to estimating quantum state overlaps that requires multiple measurements of the appropriate observables.</span></span> <span data-ttu-id="ff5d9-134">Liczba pomiarów powinna być skalowana jako $O (1/\ Epsilon ^ 2) $, gdzie $ \epsilon $ to żądany błąd szacowania.</span><span class="sxs-lookup"><span data-stu-id="ff5d9-134">The number of measurements should scale as $O(1/\epsilon^2)$ where $\epsilon$ is the desired estimation error.</span></span>
<span data-ttu-id="ff5d9-135">Jako zasada elementu kciuk liczba początkowych pomiarów może wynosić około $1/\ mbox {tolerancja} ^ 2 $ (Zobacz definicję tolerancji w poprzednim akapicie).</span><span class="sxs-lookup"><span data-stu-id="ff5d9-135">As a rule of thumb, the initial measurements count could be approximately $1/\mbox{tolerance}^2$ (see definition of tolerance in the previous paragraph).</span></span> <span data-ttu-id="ff5d9-136">Musimy skorygować liczbę pomiarów w górę, jeśli wyświetlona wartość gradientu wydaje się zbyt nieprawidłowa i zbieżność jest zbyt trudna do osiągnięcia.</span><span class="sxs-lookup"><span data-stu-id="ff5d9-136">One would need to revise the measurement count upward if the gradient descent appears to be too erratic and convergence too hard to achieve.</span></span>

### <a name="training-threads"></a><span data-ttu-id="ff5d9-137">Wątki szkoleniowe</span><span class="sxs-lookup"><span data-stu-id="ff5d9-137">Training threads</span></span>

<span data-ttu-id="ff5d9-138">Funkcja prawdopodobieństwa, która jest narzędziem szkoleniowym klasyfikatora, jest bardzo rzadko wypukła, co oznacza, że zwykle ma wiele Optima lokalnych w przestrzeni parametrów, które mogą się różnić w zależności od jakości.</span><span class="sxs-lookup"><span data-stu-id="ff5d9-138">The likelihood function which is the training utility for the classifier is very seldom convex, meaning that it usually has a multitude of local optima in the parameter space that may differ significantly by quality.</span></span> <span data-ttu-id="ff5d9-139">Ponieważ proces SGD może być zbieżny tylko z jednym określonym optymalnie, ważne jest, aby poznać wiele wektorów parametrów początkowych.</span><span class="sxs-lookup"><span data-stu-id="ff5d9-139">Since the SGD process can converge to only one specific optimum, it is important to explore multiple starting parameter vectors.</span></span> <span data-ttu-id="ff5d9-140">Typowym sposobem uczenia maszynowego jest zainicjowanie takich wektorów uruchamiania losowo.</span><span class="sxs-lookup"><span data-stu-id="ff5d9-140">Common practice in machine learning is to initialize such starting vectors randomly.</span></span> <span data-ttu-id="ff5d9-141">Q#Interfejs API uczenia akceptuje dowolną tablicę takich wektorów, ale kod źródłowy eksploruje je sekwencyjnie.</span><span class="sxs-lookup"><span data-stu-id="ff5d9-141">The Q# training API accepts an arbitrary array of such starting vectors but the underlying code explores them sequentially.</span></span> <span data-ttu-id="ff5d9-142">Na komputerze z wieloma rdzeniami lub w przypadku dowolnej architektury obliczeń równoległych zaleca się wykonanie kilku wywołań Q# interfejsu API szkolenia równolegle z innymi inicjalizacjami parametrów dla wywołań.</span><span class="sxs-lookup"><span data-stu-id="ff5d9-142">On a multicore computer or in fact on any parallel computing architecture it is advisable to perform several calls to Q# training API in parallel with different parameter initializations across the calls.</span></span>

#### <a name="how-to-modify-the-hyperparameters"></a><span data-ttu-id="ff5d9-143">Jak zmodyfikować parametry</span><span class="sxs-lookup"><span data-stu-id="ff5d9-143">How to modify the hyperparameters</span></span>

<span data-ttu-id="ff5d9-144">W bibliotece QML najlepszym sposobem modyfikowania parametrów jest zastępowanie wartości domyślnych UDT [`TrainingOptions`](xref:microsoft.quantum.machinelearning.trainingoptions) .</span><span class="sxs-lookup"><span data-stu-id="ff5d9-144">In the QML library, the best way to modify the hyperparameters is by overriding the default values of the UDT [`TrainingOptions`](xref:microsoft.quantum.machinelearning.trainingoptions).</span></span> <span data-ttu-id="ff5d9-145">W tym celu należy wywołać tę funkcję [`DefaultTrainingOptions`](xref:microsoft.quantum.machinelearning.defaulttrainingoptions) i zastosować operator `w/` w celu zastąpienia wartości domyślnych.</span><span class="sxs-lookup"><span data-stu-id="ff5d9-145">To do this we call it with the function [`DefaultTrainingOptions`](xref:microsoft.quantum.machinelearning.defaulttrainingoptions) and apply the operator `w/` to override the default values.</span></span> <span data-ttu-id="ff5d9-146">Na przykład, aby użyć 100 000 pomiarów i szybkość uczenia 0,01:</span><span class="sxs-lookup"><span data-stu-id="ff5d9-146">For example, to use 100,000 measurements and a learning rate of 0.01:</span></span>
 ```qsharp
let options = DefaultTrainingOptions()
w/ LearningRate <- 0.01
w/ NMeasurements <- 100000;
 ```
